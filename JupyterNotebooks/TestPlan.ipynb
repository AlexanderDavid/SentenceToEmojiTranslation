{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Plan\n",
    "For every algorithm developed it is important to design some test cases that the algorithm should pass. For an algorithm that generates summaries that can be slightly difficult. For systems that take input from one known language to another there are established techniques that are avalable but for our project we need to come up with something different. The solution we came up with for his is to generate test cases \"on the fly\" and prompt a user to reverse the given summary for comparison. We do this by generating (or reading in) some sentences, summarizing these sentences, and then comparing the human guess as to what the emojis mean to the input sentence. The general flow for this process is as follows\n",
    "\n",
    "   1. Generate (or read) sentences\n",
    "   2. Summarize each of the sentences\n",
    "   3. Take the top 20 sentences, sorted by the certainty score\n",
    "   4. For each machine translated sentence:\n",
    "       1. Provide the user with the emojis\n",
    "       2. Provide the user with an approximate sentence length\n",
    "       3. Prompt the user to tranlate the emojis into a sentence\n",
    "   5. For each machine translated sentence-user translated sentence pair:\n",
    "       1. Calculate the distance between the two sentences using sent2vec (might need another metric)\n",
    "\n",
    "After we have the list of numerical scores for the translations we can do some analysis on how the algorithm actually performs.\n",
    "\n",
    "### Issues\n",
    "1. Nothing to compare the cosine similarity against. Need to do some testing to determine what a \"good\" number is.\n",
    "2. The dataset we have has too many complex sentences. We should begin testing with simpler inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Generation\n",
    "The sentences are gathered from the [Stanford NLP research group's NMT dataset](https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2012.en). All of these sentences will be loaded into memory, filtered based on length, and cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentences\n",
    "file_path = \"data/tst2012.en\"\n",
    "testing_sentences = []\n",
    "with open(file_path, \"r\") as sents:\n",
    "    testing_sentences = [sent for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the sentences based on an upper and lower bound for the sentence length\n",
    "from nltk import word_tokenize\n",
    "word_limit_lower = 5\n",
    "word_limit_upper = 10\n",
    "testing_sentences = list(filter(lambda sent: len(word_tokenize(sent)) <= word_limit_upper and \n",
    "                                             len(word_tokenize(sent)) >= word_limit_lower, testing_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the sentence\n",
    "testing_sentences = [testing_sentence.replace(\"&apos;\", \"'\") for testing_sentence in testing_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 sentences in dataset\n"
     ]
    }
   ],
   "source": [
    "# Query how many sentences are in the current dataset\n",
    "print(f\"{len(testing_sentences)} sentences in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Summarization\n",
    "To do this we will just be using an exported Python V1 program that is just the NaiveEmojiTranslation notebook exported to .py. We summarize with the current best known params based on some limited observation. The sentences will be summarized and then the top thirty summarizations with the highest summarization scores are returned to prompt the user with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 : 0.07739938080495357\n",
      "50 : 0.15479876160990713\n",
      "75 : 0.23219814241486067\n",
      "100 : 0.30959752321981426\n",
      "125 : 0.38699690402476783\n",
      "150 : 0.46439628482972134\n",
      "175 : 0.541795665634675\n",
      "200 : 0.6191950464396285\n",
      "225 : 0.6965944272445821\n",
      "250 : 0.7739938080495357\n",
      "275 : 0.8513931888544891\n",
      "300 : 0.9287925696594427\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')               # cosine distance gives warnings when div by 0\n",
    "from NaiveEmojiTranslation_V1 import summarize, lemmatizerNLTK # Exported NaiveEmojiTranslation to Python file as of October 24th\n",
    "\n",
    "# Summarize each testing sentence with the current best known parameters\n",
    "summarized_sentences = []\n",
    "i = 0\n",
    "\n",
    "for sentence in testing_sentences:\n",
    "        \n",
    "    i += 1\n",
    "    if i % 25 == 0:\n",
    "        print(i, \":\", i / len(testing_sentences))\n",
    "    \n",
    "    summarized_sentences.append(summarize(sentence, keep_stop_words=True, \n",
    "                                  lemma_func=lemmatizerNLTK.lemmatize, scoring_func=scoring_function))\n",
    "\n",
    "    # TODO: While we are summarizing this massive amount of data we can also record average summarization times\n",
    "    # TODO: Print out more information (such as time) rather than just the percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence, Target Length, Your Guess\n",
      "ğŸ™†ğŸ™, 5, \n",
      "ğŸ¾âŒšï¸ğŸ®, 8, \n",
      "ğŸ‘¥ğŸ’­ğŸ†, 7, \n",
      "ğŸ‘¤ğŸ‘¥â¤ï¸ğŸš£, 6, \n",
      "ğŸƒğŸ¢, 5, \n",
      "ğŸ™, 4, \n",
      "ğŸ™, 4, \n",
      "ğŸ™, 4, \n",
      "ğŸ™…ğŸ’ğŸ‘, 6, \n",
      "ğŸš¯ğŸ’¨ğŸŒ½ğŸ”—, 8, \n",
      "ğŸ–‡ğŸ’•ğŸ‘¥, 5, \n",
      "â¤ï¸ğŸˆğŸ›, 8, \n",
      "ğŸ‘¤ğŸ™ŠğŸ‘‹, 8, \n",
      "ğŸ‘¤ğŸ”™ğŸ‘–, 6, \n",
      "ğŸ¬ğŸ‘­ğŸ, 8, \n",
      "ğŸ¾ğŸ®, 6, \n",
      "ğŸš¥ğŸŒ, 4, \n",
      "ğŸŒ›âœ‚ï¸, 3, \n",
      "ğŸ‘¤ğŸ™, 6, \n",
      "ğŸ‘¤ğŸ’­ğŸ”®, 7, \n",
      "ğŸ•¥ğŸ”®ğŸ’, 9, \n",
      "ğŸš¯ğŸˆ¸ğŸ’¡, 7, \n",
      "ğŸ’ğŸ™†, 5, \n",
      "ğŸ˜²ğŸ, 9, \n",
      "ğŸ”ğŸŒ„, 9, \n",
      "ğŸ‘¤ğŸš²ğŸ¬, 5, \n",
      "ğŸ‘¤ğŸ’, 7, \n",
      "ğŸ’­ğŸˆ¶, 5, \n",
      "ğŸ†˜ğŸ, 6, \n",
      "ğŸ‘¤ğŸ™, 9, \n",
      "ğŸ”™â¤ï¸, 6, \n",
      "ğŸ›ƒğŸ›ƒ, 6, \n",
      "ğŸš¥ğŸ‘, 6, \n",
      "ğŸ‘¤ğŸ›ƒ, 5, \n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences by their uncertainty scores. This is imported as a generic scoring\n",
    "# function so that it can be swapped in and out easily\n",
    "from NaiveEmojiTranslation_V1 import score_summarization_result_average as scoring_function\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Sort the list by the scoring function\n",
    "summarized_sentences = [x for x in summarized_sentences if len(x.uncertainty_scores) >= 1]\n",
    "summarized_sentences_sorted = list(sorted(summarized_sentences, key=lambda x: sum(x.uncertainty_scores) / len(x.uncertainty_scores)))\n",
    "\n",
    "with open('summaries.pkl', 'wb') as f:\n",
    "    pickle.dump(summarized_sentences_sorted, f)\n",
    "\n",
    "# Choose only the top 30 summaries\n",
    "testing_summaries = summarized_sentences_sorted[:34]\n",
    "\n",
    "print(\"Emoji Sequence, Target Length, Your Guess\")\n",
    "for summary in testing_summaries:\n",
    "    print(summary.emojis + \", \" + str(len(\" \".join(summary.n_grams).split(\" \"))) + \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input\n",
    "For each summarized sentence the user is given the sequence of emojis and the input sentence length. With this information the user will input their guess at the input sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NaiveEmojiTranslation_V1 import EmojiSummarizationResult\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class UserSummarization:\n",
    "    \"\"\"\n",
    "    Struct-esque data structure that stores the machines summarization and the user's guess in one object.\n",
    "    This is just syntactic sugar for a python object with some default values and type checking.\n",
    "    \"\"\"\n",
    "    machine_summarization: EmojiSummarizationResult\n",
    "    user_guess: str = \"\"\n",
    "    difference: float = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: ğŸŠğŸŒ\n",
      "Input sentence Length: 7\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? Swim to your home\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: ğŸğŸ˜†ğŸ\n",
      "Input sentence Length: 5\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? THe teacher loves this month\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: ğŸš·ğŸŒ‰\n",
      "Input sentence Length: 6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? Don't walk when it's night\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: ğŸ¦ğŸ‘–ğŸ”œ\n",
      "Input sentence Length: 4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? The banker works soon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: ğŸ§ğŸ\n",
      "Input sentence Length: 6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? She does not like ice cream\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: ğŸš·ğŸ”ŸğŸ\n",
      "Input sentence Length: 5\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? Don't walk for ten minutes every dayh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: ğŸ’•ğŸ€ğŸ\n",
      "Input sentence Length: 9\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? Love luck and weird asian pine emoji\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: ğŸ™ğŸ™Š\n",
      "Input sentence Length: 4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? Do not talk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: ğŸ™…ğŸ’‚ğŸ“¤\n",
      "Input sentence Length: 6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? Do not attack the guard\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Sequence: ğŸ’ˆğŸ§\n",
      "Input sentence Length: 8\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What's your translation? Shaving the ice cream\n"
     ]
    }
   ],
   "source": [
    "# Array for storing the users guesses\n",
    "user_summaries = []\n",
    "\n",
    "# Loop through all generated summaries\n",
    "for summary in summarized_sentences_sorted:\n",
    "    # Give the user the emoji summary and the input sentence length to shoot for in summary\n",
    "    print(f\"Emoji Sequence: {summary.emojis}\")\n",
    "    print(\"Input sentence Length: {}\".format(len(word_tokenize(\" \".join(summary.n_grams)))))\n",
    "    \n",
    "    # Prompt the user for their translation\n",
    "    translation = input(\"What's your translation?\")\n",
    "    \n",
    "    # Append a new UserSummarization object with the machines summary and the users translation to the list\n",
    "    user_summaries.append(UserSummarization(summary, translation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "To score the sentence we use sent2vec to first vectorize the sentence. This produces a 700-dimensional vector that represents the sentence as a point in space. The point is determined by the context and the content of the sentence. Two word embeddings are compared using Cosine Similarity. This measures the cosine of the angle between two vectors and is mapped from 1 - 0. With 1 being very different and 0 being exactly the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Emoji Sequence</th>\n",
       "      <th>User Guess</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[do n't those librarian swim at, home]</td>\n",
       "      <td>ğŸŠğŸŒ</td>\n",
       "      <td>Swim to your home</td>\n",
       "      <td>0.835372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[many teacher, laugh every, month]</td>\n",
       "      <td>ğŸğŸ˜†ğŸ</td>\n",
       "      <td>THe teacher loves this month</td>\n",
       "      <td>0.512813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[do n't you walk every, evening]</td>\n",
       "      <td>ğŸš·ğŸŒ‰</td>\n",
       "      <td>Don't walk when it's night</td>\n",
       "      <td>0.536855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[the banker, work, quickly]</td>\n",
       "      <td>ğŸ¦ğŸ‘–ğŸ”œ</td>\n",
       "      <td>The banker works soon</td>\n",
       "      <td>0.201708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[andrew 's uncle shave every, day]</td>\n",
       "      <td>ğŸ§ğŸ</td>\n",
       "      <td>She does not like ice cream</td>\n",
       "      <td>0.967725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[i walk for, ten, minute]</td>\n",
       "      <td>ğŸš·ğŸ”ŸğŸ</td>\n",
       "      <td>Don't walk for ten minutes every dayh</td>\n",
       "      <td>0.318119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[do those carpenter jump for two or, three, mo...</td>\n",
       "      <td>ğŸ’•ğŸ€ğŸ</td>\n",
       "      <td>Love luck and weird asian pine emoji</td>\n",
       "      <td>0.878916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[do you always, talk]</td>\n",
       "      <td>ğŸ™ğŸ™Š</td>\n",
       "      <td>Do not talk</td>\n",
       "      <td>0.636829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[do n't those, guard, fight nicely]</td>\n",
       "      <td>ğŸ™…ğŸ’‚ğŸ“¤</td>\n",
       "      <td>Do not attack the guard</td>\n",
       "      <td>0.500322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[the barber, shave for more than an hour]</td>\n",
       "      <td>ğŸ’ˆğŸ§</td>\n",
       "      <td>Shaving the ice cream</td>\n",
       "      <td>0.869723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input Emoji Sequence  \\\n",
       "0             [do n't those librarian swim at, home]             ğŸŠğŸŒ   \n",
       "1                 [many teacher, laugh every, month]            ğŸğŸ˜†ğŸ   \n",
       "2                   [do n't you walk every, evening]             ğŸš·ğŸŒ‰   \n",
       "3                        [the banker, work, quickly]            ğŸ¦ğŸ‘–ğŸ”œ   \n",
       "4                 [andrew 's uncle shave every, day]             ğŸ§ğŸ   \n",
       "5                          [i walk for, ten, minute]            ğŸš·ğŸ”ŸğŸ   \n",
       "6  [do those carpenter jump for two or, three, mo...            ğŸ’•ğŸ€ğŸ   \n",
       "7                              [do you always, talk]             ğŸ™ğŸ™Š   \n",
       "8                [do n't those, guard, fight nicely]            ğŸ™…ğŸ’‚ğŸ“¤   \n",
       "9          [the barber, shave for more than an hour]             ğŸ’ˆğŸ§   \n",
       "\n",
       "                              User Guess  Difference  \n",
       "0                      Swim to your home    0.835372  \n",
       "1           THe teacher loves this month    0.512813  \n",
       "2             Don't walk when it's night    0.536855  \n",
       "3                  The banker works soon    0.201708  \n",
       "4            She does not like ice cream    0.967725  \n",
       "5  Don't walk for ten minutes every dayh    0.318119  \n",
       "6   Love luck and weird asian pine emoji    0.878916  \n",
       "7                            Do not talk    0.636829  \n",
       "8                Do not attack the guard    0.500322  \n",
       "9                  Shaving the ice cream    0.869723  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Difference: 0.6258380822837353\n"
     ]
    }
   ],
   "source": [
    "from NaiveEmojiTranslation_V1 import s2v\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results[\"Input\"] = [summary.machine_summarization.n_grams for summary in user_summaries]\n",
    "results[\"Emoji Sequence\"] = [\"\".join(summary.machine_summarization.emojis) for summary in user_summaries]\n",
    "results[\"User Guess\"] = [summary.user_guess for summary in user_summaries]\n",
    "results[\"Difference\"] = [cosine(s2v.embed_sentence(summary.user_guess), s2v.embed_sentence(\" \".join(summary.machine_summarization.n_grams))) for summary in user_summaries]\n",
    "\n",
    "from IPython.display import display\n",
    "display(results)\n",
    "\n",
    "print(\"Average Difference: {}\".format(sum(results[\"Difference\"])/len(results[\"Difference\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
