{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Sentence to Emoji Translation \n",
    "## Purpose\n",
    "To workshop a naive version of an sentence to emoji translation algorithm. The general idea is that sentences can be \"chuncked\" out into n-grams that are more related to a single emoji. The related-ness of an n-gram to an emoji is directly related to the cosine similarity of the sent2vec representation of the sentence and the sent2vec representation of one of the emoji's definitions. The emoji definitons are gathered from the [emoji2vec](https://github.com/uclmr/emoji2vec) github repo and the sent2vec model is from the [sent2vec](https://github.com/epfml/sent2vec) github repo. \n",
    "\n",
    "## Issues\n",
    "- The generation of the summary is so incredibly slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/b9/01/fcb8ae3e836fea5c11fdb4c074d27b52bdf74b47bd9bb28a811b7ab37d49/spacy-2.2.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting plac<1.2.0,>=0.9.6 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata>=0.20; python_version < \"3.8\" (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/f6/d2/40b3fa882147719744e6aa50ac39cf7a22a913cbcba86a0371176c425a3b/importlib_metadata-0.23-py2.py3-none-any.whl\n",
      "Collecting srsly<1.1.0,>=0.1.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/00/5a/9a3288b648a0c5c86d0a2ef972b0a8062ff1088658da8165b370564ef346/srsly-0.2.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy) (41.0.1)\n",
      "Collecting blis<0.5.0,>=0.4.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/0a/8c/f1b2aad385de78db151a6e9728026f311dee8bd480f2edc28a0175a543b6/blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fc/10eeacb926ec1e88cd62f79d9ac106b0a3e3fe5ff1690422d88c29bd0909/murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/6c/5b/ae4da6230eb48df353b199f53532c8407d0e9eb6ed678d3d36fa75ac391c/preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting wasabi<1.1.0,>=0.3.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/3f/35/6dc35bc3b49e160a016e420eb4bdcf1c887db6fd33a463959c06a508c339/wasabi-0.4.0-py3-none-any.whl\n",
      "Collecting thinc<7.4.0,>=7.3.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/32/53/d11d2faa6921e55c37ad2cd56b0866a9e6df647fb547cfb69a50059d759c/thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/26/e534148e509cbebbea3ee29f50f59eb206621d12c35e4594507da8dc54cc/cymem-2.0.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.17.2)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/74/3d/1ee25a26411ba0401b43c6376d2316a71addcc72ef8690b101b4ea56d76a/zipp-0.6.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.32.2)\n",
      "Collecting more-itertools (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/dc/3241eef99eb45f1def35cf93af35d1cf9ef4c0991792583b8f33ea41b092/more_itertools-7.2.0-py3-none-any.whl\n",
      "Installing collected packages: plac, more-itertools, zipp, importlib-metadata, srsly, blis, murmurhash, cymem, preshed, wasabi, thinc, spacy\n",
      "Successfully installed blis-0.4.1 cymem-2.0.2 importlib-metadata-0.23 more-itertools-7.2.0 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.2.2 srsly-0.2.0 thinc-7.3.1 wasabi-0.4.0 zipp-0.6.0\n",
      "Collecting tabulate\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.8.5\n",
      "Processing /home/jovyan/work/sent2vec\n",
      "Building wheels for collected packages: sent2vec\n",
      "  Building wheel for sent2vec (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sent2vec: filename=sent2vec-0.0.0-cp37-cp37m-linux_x86_64.whl size=1174126 sha256=f423d87acc99e0ee14e21b42064f2391c7a3b52906c138162c96b9f1eaa133c0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-33jvsdex/wheels/d9/e7/26/888a49dfb31f02930dae17493fe08a50bfd1fb09ab9959ea60\n",
      "Successfully built sent2vec\n",
      "Installing collected packages: sent2vec\n",
      "Successfully installed sent2vec-0.0.0\n"
     ]
    }
   ],
   "source": [
    "# Installs TODO: Add these to docke\n",
    "!pip install spacy\n",
    "!pip install tabulate\n",
    "!pip install ../../sent2vec/\n",
    "\n",
    "# Standard Library\n",
    "from typing import List, Tuple, Callable # Datatypes for the function typing\n",
    "from functools import lru_cache         # Function annotation for storing results \n",
    "from dataclasses import dataclass, field # C-like struct functions and class annotation\n",
    "from string import punctuation\n",
    "\n",
    "# Scipy suite\n",
    "import numpy as np                       # For function annotation\n",
    "from scipy.spatial.distance import cosine # Distance between sentence and emoji in sent2vec vector space\n",
    "\n",
    "# NLTK\n",
    "from nltk import word_tokenize, pos_tag                                # Tokenizing a sentence into words and tagging POS\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer # Different stemming algorithms\n",
    "from nltk.corpus import stopwords                                      # Define the set of stopwords in english\n",
    "from nltk import Tree\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Import spacy (NLP)\\n\",\n",
    "import spacy\n",
    "\n",
    "# Import sentence vectorizer\\n\"\n",
    "import sent2vec\n",
    "\n",
    "# IPython output formatting\\n\",\n",
    "from tabulate import tabulate                           # Tabulation from 2-d array into html table\n",
    "from IPython.display import display, HTML, clear_output # Nice displaying in the output cell\n",
    "import warnings; warnings.simplefilter('ignore')        # cosine distance gives warnings when div by 0 so\n",
    "                                                        # ignore all of these\n",
    "# Timing functions\n",
    "from time import time, localtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramatize the file locations\n",
    "emoji_file = \"../data/emoji_joined.txt\" # https://github.com/uclnlp/emoji2vec/blob/master/data/raw_training_data/emoji_joined.txt\n",
    "wikipedia_file = \"../data/wikipedia_utf8_filtered_20pageviews.csv\" # https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sent2vec model\n",
    "s2v = sent2vec.Sent2vecModel()\n",
    "s2v.load_model('../../models/wiki_unigrams.bin') # https://drive.google.com/open?id=0B6VhzidiLvjSa19uYWlLUEkzX3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz#egg=en_core_web_sm==2.2.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.0MB 5.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.2.0) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.17.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (41.0.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.23)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (7.3.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.32.2)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (7.2.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-cp37-none-any.whl size=12019126 sha256=2ef90413f1db0d2bf49d6848aee776d13b80d58eb872ef73edeea655b7b54175\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-55mlzl_q/wheels/48/5c/1c/15f9d02afc8221a668d2172446dd8467b20cdb9aef80a172a4\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m‚úî Linking successful\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "# Intitialize the lemmatizers\n",
    "!python -m spacy download en\n",
    "ps = PorterStemmer()\n",
    "sb = SnowballStemmer(\"english\")\n",
    "lemmatizerNLTK = WordNetLemmatizer()\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Cleaning\n",
    "The general idea with sentence cleaning is that the sentences need to be put into the same \"format\" for better analysis. There are two main aspects of cleaning: 1) removal, and 2) modification. Removal is primarily for tokens that do not contribute to the sentence at all. These include \".\", \"and\", \"but\". Normally this is a standard step in sentence cleaning but it has actually has zero effect on the output that I can see. However, token modification changes the spelling of tokens to uniform all tokens that use the same root. For example \"rocked\", \"rock\", \"rocking\" should all be reduced to their lemma of \"rock\". There are two different ways to do this: [stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent: str, lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True) -> str:\n",
    "    \"\"\"\n",
    "    Clean a sentence\n",
    "    \n",
    "    Tokenize the word and then lemmatize each individual word before rejoining it all together.\n",
    "    Optionally removing stop words along the way\n",
    "        \n",
    "    Args:\n",
    "        sent(str): Sentence to clean\n",
    "        lemma_func(Callable[[str], str]): A function that takes in a word and outputs a word,\n",
    "                                          normally used to pass in the lemmatization function to be mapped\n",
    "                                          on every word the sentence\n",
    "        keep_stop_words(bool): Keep the stop words in the sentence\n",
    "    Rets:\n",
    "        (str): Cleaned sentence\n",
    "    \"\"\"\n",
    "    # Lemmatize each word in the sentence and remove the stop words if the flag is set\n",
    "    return \" \".join([lemma_func(token) for token in word_tokenize(sent.lower()) if (token not in stopwords or keep_stop_words) and (token not in punctuation)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emoji Vectorization and Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the array to store the (emoji, repr) 2-tuple\n",
    "def generate_emoji_embeddings(lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True) -> List[Tuple[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Generate the sent2vec emoji embeddings from the input file\n",
    "    \n",
    "    Run each emoji within the emoji_joined data file from the emoji2vec paper through\n",
    "    the sent2vec sentence embedder. This is a very naive way of doing it because one\n",
    "    emoji may have multiple entries in the data file so it has multiple vectors in the\n",
    "    emoji_embeddings array\n",
    "    \n",
    "    Args:\n",
    "        lemma_func(Callable[[str], str]): Lemmatization function for cleaning. A function that takes in a word and outputs a word,\n",
    "                                          normally used to pass in the lemmatization function to be mapped\n",
    "                                          on every word the sentence\n",
    "        keep_stop_words(bool): Keep the stop words in the cleaned sentence\n",
    "    Rets:\n",
    "        (List[Tuple[str, List[float]]]): A list of 2-tuples containing the emoji and \n",
    "                                         one vector representation of it\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list that will hold all of the embedings\n",
    "    emoji_embeddings = []\n",
    "    \n",
    "    # Open the file that stores the emoji, description 2-tuple list\n",
    "    with open(emoji_file) as emojis:\n",
    "        for defn in emojis:\n",
    "            # The file is tab-delim\n",
    "            split = defn.split(\"\\t\")\n",
    "\n",
    "            # Get the emoji and the description from the current line\n",
    "            emoji = split[-1].replace(\"\\n\", \"\")\n",
    "            desc = clean_sentence(split[0], lemma_func, keep_stop_words)\n",
    "\n",
    "            # Add each emoji and embedded description to the list\n",
    "            emoji_embeddings.append((emoji, s2v.embed_sentence(desc), desc))\n",
    "            \n",
    "    # Return the embeddings\n",
    "    return emoji_embeddings\n",
    "\n",
    "emoji_embeddings = generate_emoji_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1000)\n",
    "def closest_emoji(sent: str) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Get the closest emoji to the given sentence\n",
    "    \n",
    "    Loop through the list of emoji embeddings and keep track of which one has the\n",
    "    lowest cosine distance from the input sentence's embedding. This is the \"closest\"\n",
    "    emoji. The lru_cache designation means that python will store the last [maxsize]\n",
    "    calls to this function with their return value to reduce computation. This is\n",
    "    cleared after every call to the summary function.\n",
    "    \n",
    "    Args:\n",
    "        sent(List[str]): Sentence to check\n",
    "    Ret:\n",
    "        (Tuple[str, int]) Closest emoji, cosine similarity of emoji\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Embed the sentence using sent2vec \n",
    "    emb = s2v.embed_sentence(sent)\n",
    "\n",
    "    # Start the lowest cosine at higher than it could ever be\n",
    "    lowest_cos = 1_000_000\n",
    "\n",
    "    # The best emoji starts as an empty string placeholder\n",
    "    best_emoji = \"\"\n",
    "    best_ngram = \"\"\n",
    "\n",
    "    # Loop through the dictionary\n",
    "    for emoji in emoji_embeddings:\n",
    "        # Get the current emoji's embedding\n",
    "        emoji_emb = emoji[1]\n",
    "\n",
    "        # Check the cosine difference between the emoji's embedding and\n",
    "        # the sentence's embedding\n",
    "        curr_cos = cosine(emoji_emb, emb)\n",
    "\n",
    "        # If it lower than the lowest then it is the new best\n",
    "        if curr_cos < lowest_cos:\n",
    "            lowest_cos = curr_cos\n",
    "            best_emoji = emoji[0]\n",
    "            best_ngram = emoji[2]\n",
    "\n",
    "    # Return a 2-tuple containing the best emoji and its cosine differnece\n",
    "    return best_emoji, lowest_cos, best_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram Generation and Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_n_gram(sentence: str, keep_stop_words: bool=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate an n-gram based on the POS tagged dependency tree of the sentence that is \"simplified\" down according\n",
    "    to a few assumptions that dictate a good sentence split. These assumptions are as follows:\n",
    "        1. If two words are leafs and on the same level with the same parent they can be grouped as an n-gram\n",
    "        2. If there is a sequence of parent-child relationships with only 1 child they can be grouped as one\n",
    "           n-gram\n",
    "           \n",
    "    \n",
    "    \"\"\"\n",
    "    stopword = \"the in has be\".split()\n",
    "    pos_tagged_n_grams = []\n",
    "    \n",
    "    def to_nltk_tree(node):\n",
    "        current_node = node\n",
    "        backlog = []\n",
    "        while current_node.n_lefts + current_node.n_rights == 1:\n",
    "            backlog.append((current_node.orth_, current_node.i))\n",
    "            current_node = list(current_node.children)[0]\n",
    "\n",
    "        backlog.append((current_node.orth_, current_node.i))\n",
    "        if current_node.n_lefts + current_node.n_rights > 1:\n",
    "            good_children = [child for child in current_node.children if len(list(child.children)) > 0]\n",
    "            bad_children = [(child.orth_, child.i) for child in current_node.children if child not in good_children]\n",
    "            pos_tagged_n_grams.append(backlog)\n",
    "            pos_tagged_n_grams.append(bad_children)\n",
    "            return Tree(backlog, [Tree(bad_children, [])] + [to_nltk_tree(child) for child in good_children])\n",
    "        else:\n",
    "            pos_tagged_n_grams.append(backlog)\n",
    "            return Tree(backlog, [])\n",
    "        \n",
    "    def strip_nothing_unigrams(n_grams):\n",
    "        return [n_gram for n_gram in n_grams if not (len(n_gram.split(\" \")) == 1 and n_gram.split(\" \")[0] in stopword)]\n",
    "\n",
    "#     query = \" \".join([word for word in sentence.split() if word not in stopword or keep_stop_words])\n",
    "    doc = nlp(sentence)\n",
    "    to_nltk_tree(list(doc.sents)[0].root);\n",
    "    # print(nltk_tree)\n",
    "\n",
    "    sort_inner = [sorted(nltk_child, key=lambda x: x[1]) for nltk_child in pos_tagged_n_grams]\n",
    "\n",
    "    nltk_averages = []\n",
    "    for nltk_child in sort_inner:\n",
    "        if nltk_child == []:\n",
    "            continue\n",
    "        nltk_averages.append((nltk_child, max(x[1] for x in nltk_child)))\n",
    "\n",
    "    sorted_outer = list(sorted(nltk_averages, key=lambda x: x[1]))\n",
    "\n",
    "    n_grams = []\n",
    "    for nltk_average in sorted_outer:\n",
    "        n_grams.append(\" \".join(word[0] for word in nltk_average[0]))\n",
    "        \n",
    "    return n_grams\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_n_gram(n_grams:List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that a given n_gram is good. Good is defined as the series of n-grams contains no n-grams containing only stop words\n",
    "    \"\"\"\n",
    "    stopwords = \"the and but\".split()\n",
    "    return list(filter(lambda x: x not in stopwords, n_grams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations_of_sent(sent: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Return all possible n-gram combinations of a sentence\n",
    "    \n",
    "    Args:\n",
    "        sent(str): Sentence to n-gram-ify\n",
    "    Rets:\n",
    "        (List[List[str]]): List of all possible n-gram combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    def combinations_of_sum(sum_to: int, combo: List[int]=None) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Return all possible combinations of ints that sum to some int\n",
    "        \n",
    "        Args:\n",
    "            sum_to(int): The number that all sub-arrays should sum to\n",
    "            combo(List[int]): The current combination of number that the recursive\n",
    "                              algo should subdivide, not needed for first run but used\n",
    "                              in every consequent recursive run of the function\n",
    "        \"\"\"\n",
    "        # Initialize the list for combinations\n",
    "        combos = []\n",
    "        \n",
    "        # If the current combo list is none (first run through)\n",
    "        # then generate it with all 1s and length = sum_to\n",
    "        if combo is None:\n",
    "            combo = [1 for x in range(sum_to)]\n",
    "            combos.append(combo)\n",
    "\n",
    "        # Base case: If the length  of the combination is 0 then\n",
    "        # end the recursion because we are at the top of the \"tree\"\n",
    "        if len(combo) == 0:\n",
    "            return None\n",
    "\n",
    "        # For each \n",
    "        for i in range(1, len(combo)):\n",
    "            combo_to_query = combo[:i-1] + [sum(combo[i - 1:i + 1])] + combo[i+1:]\n",
    "            combos.append(combo_to_query)\n",
    "            [combos.append(combo) for combo in combinations_of_sum(sum_to, combo_to_query) if combo is not None]\n",
    "\n",
    "        return combos\n",
    "    \n",
    "    def combinations_of_sent_helper(sent):\n",
    "        sent = word_tokenize(sent)\n",
    "        combos = np.unique(combinations_of_sum(len(sent)))\n",
    "        sent_combos = []\n",
    "        for combo in combos:\n",
    "            sent_combo = []\n",
    "            curr_i = 0\n",
    "            for combo_len in combo:\n",
    "                space_joined = \" \".join(sent[curr_i:combo_len + curr_i])\n",
    "                if space_joined not in sent_combo:\n",
    "                    sent_combo.append(space_joined) \n",
    "                curr_i += combo_len\n",
    "\n",
    "            if sent_combo not in sent_combos:\n",
    "                sent_combos.append(sent_combo)\n",
    "        return sent_combos\n",
    "    \n",
    "    return combinations_of_sent_helper(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization Algorithm and Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EmojiSummarizationResult:\n",
    "    \"\"\"\n",
    "    \"Struct\" for keeping track of an Emoji Summarization result\n",
    "    \n",
    "    Data Members:\n",
    "        emojis(str): String of emojis that represent the summarization\n",
    "        n_grams(List[str]): List of variable length n-grams that each emoji represents\n",
    "        uncertainty_scores(List[float]): List of the cosine distance between each n_gram and emoji\n",
    "        time_elapsed(float): How long it took to complete the summary\n",
    "    \"\"\"\n",
    "    emojis: str = \"\"\n",
    "    emojis_n_grams: str = field(default_factory=list)\n",
    "    n_grams: List[str] = field(default_factory=list)\n",
    "    uncertainty_scores: List[float] = field(default_factory=list)\n",
    "    elapsed_time: float = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### CHRIS'S ALGORITHMS FROM FACEBOOK \n",
    "#Note: I'm pretty sure that these means exist in numpy.\n",
    "import numpy\n",
    "\n",
    "\n",
    "\n",
    "# Can do with logs - better?\n",
    "def weightedGeometricAverage(uncertainty_scores, n_grams):\n",
    "  weightedProduct = 1\n",
    "  sentenceLength = 0 # Should never be 0\n",
    "  for i in range(len(uncertainty_scores)):\n",
    "    sentenceLength += len(n_grams.split(\" \")[i])\n",
    "    weightedProduct *= uncertainty_scores[i]**len(n_grams.split(\" \")[i])\n",
    "  return weightedProduct ** (1/sentenceLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted on real estate an n-gram occupies\n",
    "def score_summarization_result_weighted_average(summarization: EmojiSummarizationResult) -> float:\n",
    "    weighted_sum = 0\n",
    "    sentence_length = 0\n",
    "    for i in range(len(summarization.uncertainty_scores)):\n",
    "        sentence_length += len(summarization.n_grams[i].split(\" \"))\n",
    "        weighted_sum += summarization.uncertainty_scores[i] * len(summarization.n_grams[i].split(\" \"))\n",
    "  \n",
    "    return weighted_sum/sentence_length\n",
    "\n",
    "def score_summarization_result_geometric_average(summarization: EmojiSummarizationResult) -> float:\n",
    "    return np.prod(summarization.uncertainty_scores)**(1/len(summarization.uncertainty_scores))\n",
    "\n",
    "# Can do with logs - better?\n",
    "def score_summarization_result_weighted_geometric_average(summarization: EmojiSummarizationResult) -> float:\n",
    "    weighted_prod = 1\n",
    "    sentence_length = 0\n",
    "    for i in range(len(summarization.uncertainty_scores)):\n",
    "        sentence_length += len(summarization.n_grams[i].split(\" \"))\n",
    "        weighted_prod += summarization.uncertainty_scores[i] ** len(summarization.n_grams[i].split(\" \"))\n",
    "        \n",
    "    return weighted_prod ** (1/sentence_length)\n",
    "\n",
    "def score_summarization_result_harmonic_average(summarization: EmojiSummarizationResult) -> float:\n",
    "    return len(summarization.n_grams) / sum([1/uncertainty_score for uncertainty_score in summarization.uncertainty_scores])\n",
    "\n",
    "def score_summarization_result_weighted_harmonic_average(summarization: EmojiSummarizationResult) -> float:\n",
    "    total = 0\n",
    "    for i in range(len(summarization.uncertainty_scores)):\n",
    "        total += 1/(len(summarization.n_grams[i].split(\" \")) * summarization.uncertainty_scores[i])\n",
    "        \n",
    "    return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_summarization_result_average(summarization: EmojiSummarizationResult) -> float:\n",
    "    \"\"\"\n",
    "    Score a EmojiSummarizationResult\n",
    "    \n",
    "    Get the average of all uncertainty scores and return that as the score\n",
    "    \n",
    "    Args:\n",
    "        summarization(EmojiSummarizationResult): Summarization to score\n",
    "        \n",
    "    Rets:\n",
    "        (float): Numerical summarization score\n",
    "    \"\"\"\n",
    "    return sum(summarization.uncertainty_scores) / len(summarization.uncertainty_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(sent:str, lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, \n",
    "              keep_stop_words: bool=True, scoring_func: Callable[[EmojiSummarizationResult], float]=score_summarization_result_average) -> EmojiSummarizationResult: \n",
    "    \"\"\"\n",
    "    Summarize the given sentence into emojis\n",
    "    \n",
    "    Split the sentence into every possible combination of n-grams and see which returns the highest score\n",
    "    when each n-gram is translated to an emoji using the closest emoji in the dataset\n",
    "    \n",
    "    Args:\n",
    "        sent(str): Sentence to summarize\n",
    "        lemma_func(Callable[[str], str]): Lemmatization function for cleaning. A function that takes in a word and outputs a word,\n",
    "                                          normally used to pass in the lemmatization function to be mapped\n",
    "                                          on every word the sentence\n",
    "        keep_stop_words(bool): Keep the stop words in the cleaned sentence\n",
    "    Rets:\n",
    "        (Tuple[List[str], List[float], List[str]]): (Emoji Sentence, \n",
    "        List of Uncertainty values for the corresponding emoji,\n",
    "        list of n-grams used to generate the corresponding emoji)\n",
    "    \"\"\"\n",
    "    # Clean the sentence\n",
    "    sent = clean_sentence(sent, lemma_func=lemma_func, keep_stop_words=keep_stop_words)\n",
    "#     print(sent)\n",
    "    \n",
    "    # Generate all combinations of sentences\n",
    "    sent_combos = combinations_of_sent(sent)\n",
    "    # Init \"best\" datamembers as empty or exceedingly high\n",
    "    best_summarization = EmojiSummarizationResult()\n",
    "    best_summarization_score = 100_000_000\n",
    "    # Iterate through every combination of sentence combos\n",
    "    for sent_combo in sent_combos:\n",
    "        # Start the local data members as empty\n",
    "        local_summarization = EmojiSummarizationResult()\n",
    "        # Iterate through each n_gram adding the uncertainty and emoji to the lists\n",
    "        for n_gram in sent_combo:\n",
    "            close_emoji, cos_diff, close_ngram = closest_emoji(n_gram)\n",
    "            local_summarization.emojis += close_emoji\n",
    "            local_summarization.uncertainty_scores.append(cos_diff)\n",
    "            local_summarization.emojis_n_grams.append(close_ngram)\n",
    "        \n",
    "        local_summarization.n_grams = sent_combo\n",
    "\n",
    "        # Check if the average uncertainty is less than the best\n",
    "        # TODO: Maybe a median check would be helpful as well?\n",
    "        if scoring_func(local_summarization) < best_summarization_score:\n",
    "            # Update the best emojis\n",
    "            best_summarization = local_summarization\n",
    "            best_summarization_score = scoring_func(best_summarization)\n",
    "            \n",
    "    # Clear the function cache on closest_emoji because it is unlikely the next run will make use of them\n",
    "    closest_emoji.cache_clear()\n",
    "    \n",
    "    # Return the emoji \"sentence\", list of all the cosine similarities, and all of the n-grams\n",
    "    return best_summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pos(sent:str, keep_stop_words:bool=False, lemma_func: Callable[[str], str]=lambda x: x) -> EmojiSummarizationResult:\n",
    "    \"\"\"\n",
    "    Summarize a sentence using POS n-gram chunking\n",
    "    \n",
    "    Args:\n",
    "        sent(str): Sentence to summarize\n",
    "        keep_stop_words(bool, Optional): Flag to keep the stop words when cleaning the sentence and n-grams\n",
    "        lemma_func(Callable[[str], str], Optional): Function to use to lemmatize the sentence\n",
    "        \n",
    "    Rets:\n",
    "        EmojiSummarizationResult: Result of the emoji summarization\n",
    "    \"\"\"\n",
    "    # Clean the sentence\n",
    "    sent = clean_sentence(sent, keep_stop_words=True, lemma_func=lemma_func)\n",
    "    \n",
    "    # Get the n-grams using the part of speech tagging\n",
    "    pos_n_grams = pos_n_gram(sent, keep_stop_words=keep_stop_words)\n",
    "    \n",
    "    # Clean the n_grams\n",
    "    n_grams = clean_n_gram(pos_n_grams)\n",
    "    \n",
    "    # Create an Emoji Summarization Result\n",
    "    esr = EmojiSummarizationResult()\n",
    "    \n",
    "    # Translate every n_gram in that n-gram sequence\n",
    "    for n_gram in n_grams:\n",
    "        # Get the closest emoji to the current n-gram\n",
    "        emoji, similarity, desc = closest_emoji(n_gram)\n",
    "        \n",
    "        # Add the closest emoji to the sumary\n",
    "        esr.emojis += emoji\n",
    "        esr.emojis_n_grams.append(desc)\n",
    "        esr.n_grams.append(n_gram)\n",
    "        esr.uncertainty_scores.append(similarity)\n",
    "\n",
    "    # Return the summary\n",
    "    return esr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification and Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_summary(sents: List[str], lemma_func: Callable[[str], str]=lemmatizerNLTK.lemmatize, keep_stop_words: bool=True, generate_embeddings: bool=True,\n",
    "                  scoring_func: Callable[[EmojiSummarizationResult], float]=score_summarization_result_average) -> HTML:\n",
    "    \"\"\"\n",
    "    Summarize a collection of sentences and display it nicely with IPython\n",
    "    \n",
    "    Args:\n",
    "        sents(List[str]): List of sentences to translate\n",
    "        lemma_func(Callable[[str], str]), optional: Lemmatization function for cleaning. A function that takes in a word and outputs a word,\n",
    "                                          normally used to pass in the lemmatization function to be mapped\n",
    "                                          on every word the sentence\n",
    "        keep_stop_words(bool), optional: Keep the stop words in the cleaned sentence\n",
    "        generate_embeddings(bool), optional: Regenerate the emoji embeddings for the case that the lemmatazation/stop_word params have changed\n",
    "        \n",
    "    Rets:\n",
    "        IPython.HTML: HTML List to be displayed with IPython\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Generate emoji embeddings in case the cleaning parameters have changed\n",
    "    if generate_embeddings:\n",
    "        time_now = time()\n",
    "        global emoji_embeddings\n",
    "        emoji_embeddings = generate_emoji_embeddings(lemma_func, keep_stop_words)\n",
    "        print(\"Completed emoji embeddings, time elapsed: {}\\n\".format(time() - time_now))\n",
    "    \n",
    "    # Create the 2d array for the talbe\n",
    "    table = []\n",
    "    \n",
    "    # Iterate through each sentence to be summarized\n",
    "    for sent in sents:\n",
    "        # Start timer\n",
    "        time_now = time()\n",
    "        \n",
    "        # Summarize it\n",
    "        summarization_res = summarize(sent, lemma_func, keep_stop_words, scoring_func)\n",
    "        \n",
    "        # Get elapsed time\n",
    "        elapsed_time = time() - time_now\n",
    "        \n",
    "        # Update elapsed time\n",
    "        summarization_res.elapsed_time = elapsed_time\n",
    "        \n",
    "        # Print status update\n",
    "        # print(\"Completed sentence: {}, time elapsed: {}\".format(sents.index(sent), elapsed_time))\n",
    "\n",
    "        # Append pertinent data to the table\n",
    "        table.append([sent, round(scoring_func(summarization_res), 3), \n",
    "                           [round(x, 3) for x in summarization_res.uncertainty_scores],\n",
    "                           summarization_res.n_grams, \n",
    "                           summarization_res.elapsed_time,\n",
    "                           summarization_res.emojis])\n",
    "        \n",
    "        # Print out an update\n",
    "    \n",
    "    # Return the table with the headers\n",
    "    return tabulate(table, tablefmt='pipe', \n",
    "                                headers=[\"Input Sentence\", \"Summary Score\", \"Individual N-Gram Scores\", \n",
    "                                         \"N-Grams\", \"Elapsed Time\", \"Emoji Results\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis: üçéüéΩüí®\n",
      "Emoji n-grams: teacher, run, fast\n",
      "Sentence n-grams: teacher, run, fast\n",
      "Uncertainty Scores: 0.0, 0.0, 0.0\n",
      "Score: 0.0\n",
      "\n",
      "Emojis: üí≠üíæüêõ\n",
      "Emoji n-grams: think, computer, virus\n",
      "Sentence n-grams: think, computer, virus\n",
      "Uncertainty Scores: 0.0, 0.0, 0.0\n",
      "Score: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pp(esr):\n",
    "    print(\"Emojis: \" + esr.emojis)\n",
    "    print(\"Emoji n-grams: \" + \", \".join(esr.emojis_n_grams))\n",
    "    print(\"Sentence n-grams: \" + \", \".join(esr.n_grams))\n",
    "    print(\"Uncertainty Scores: \" + \", \".join([str(round(x, 3)) for x in esr.uncertainty_scores]))\n",
    "    print(\"Score: \" + str(round(score_summarization_result_average(esr), 3)))\n",
    "    print()\n",
    "    \n",
    "sentences = [\"The teacher runs fast\", \"I think that this computer has a virus\"]\n",
    "for sentence in sentences:\n",
    "#     pp(summarize_pos(sentence, keep_stop_words=True))\n",
    "    pp(summarize(sentence, keep_stop_words=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmojiSummarizationResult(emojis='üçèüì±üíæ', emojis_n_grams=['grevenstein apple', 'cell phone', 'computer'], n_grams=['company apple', 'make cell phone', 'computer'], uncertainty_scores=[0.16125917434692383, 0.13012969493865967, 0.0], elapsed_time=0)\n",
      "0.9028703769048055\n"
     ]
    }
   ],
   "source": [
    "esr = summarize(\"The company apple makes both cell phones and computers\", keep_stop_words=False)\n",
    "print(esr)\n",
    "print(1 - score_summarization_result_average(esr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis: üöØüîâüî¢üôäü¶è\n",
      "Emoji n-grams: can, reduce volume, number, verbalize, rhinoceros\n",
      "Sentence n-grams: can you, calculate, number, that have ever, of giraffes existed\n",
      "Uncertainty Scores: 0.204, 0.721, 0.0, 0.848, 0.741\n",
      "Score: 0.503\n",
      "\n",
      "Emojis: üöØüîü\n",
      "Emoji n-grams: can, number 10\n",
      "Sentence n-grams: can, you calculate the number of giraffe that have ever existed\n",
      "Uncertainty Scores: 0.0, 0.692\n",
      "Score: 0.346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent = \"Can you calculate the number of giraffes that have ever existed?\"\n",
    "pp(summarize_pos(sent))\n",
    "pp(summarize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
